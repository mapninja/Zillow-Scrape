{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "\n",
    "In this tutorial, we'll scrape Zillow using Python with two community packages:\n",
    "\n",
    "- [httpx](https://pypi.org/project/httpx/) - HTTP client library to get Zillow data in either HTML or JSON.\n",
    "- [parsel](https://pypi.org/project/parsel/) - HTML parsing library to parse our web scraped HTML files.\n",
    "\n",
    "Optionally, we'll also use [loguru](https://pypi.org/project/loguru/), a logging library that will allow us to track our Zillow data scraper.  \n",
    "These packages can be installed using the following pip command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install httpx parsel loguru h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Scrape Zillow Property Pages?\n",
    "\n",
    "To start, let's explore scraping Zillow data from property pages. First, let's locate the data on the HTML from a given Zillow page, like [this one](https://www.zillow.com/b/1625-e-13th-st-brooklyn-ny-5YGKWY/).\n",
    "\n",
    "To scrape this page data, we can parse every detail using XPath or CSS selectors. However, there is a better approach: hidden web data. To find this data, follow the below steps:\n",
    "\n",
    "- Open the [browser developer tools](https://scrapfly.io/blog/browser-developer-tools-in-web-scraping/) by pressing the `F12` key.\n",
    "- Search for the selector `//script[@id='__NEXT_DATA__']`.\n",
    "\n",
    "After following the above steps, you will find the property dataset hidden in the JavaScript variable with the above XPath selector:\n",
    "\n",
    "![capture of page source of Zillow's property page](https://scrapfly.io/blog/content/images/how-to-scrape-zillow_page-source-prop.svg)\n",
    "\n",
    "We can see property data is available as JSON object in a script tag\n",
    "\n",
    "The above real estate data is the same on the page but before getting rendered into the HTML, commonly known as hidden web data.\n",
    "\n",
    "Let's power our Zillow data scraper with requesting and parsing logic for property pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "import httpx\n",
    "import h2\n",
    "import json\n",
    "from parsel import Selector\n",
    "\n",
    "client = httpx.AsyncClient(\n",
    "    # enable http2\n",
    "    http2=True,\n",
    "    # add basic browser like headers to prevent being blocked\n",
    "    headers={\n",
    "        \"accept-language\": \"en-US,en;q=0.9\",\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"accept-language\": \"en-US;en;q=0.9\",\n",
    "        \"accept-encoding\": \"gzip, deflate, br\",\n",
    "    },\n",
    ")\n",
    "\n",
    "async def scrape_properties(urls: List[str]):\n",
    "    \"\"\"scrape zillow property pages for property data\"\"\"\n",
    "    to_scrape = [client.get(url) for url in urls]\n",
    "    results = []\n",
    "    for response in asyncio.as_completed(to_scrape):\n",
    "        response = await response\n",
    "        assert response.status_code == 200, \"request has been blocked\"\n",
    "        selector = Selector(response.text)\n",
    "        data = selector.css(\"script#__NEXT_DATA__::text\").get()\n",
    "        if data:\n",
    "            # Option 1: some properties are located in NEXT DATA cache\n",
    "            data = json.loads(data)\n",
    "            property_data = json.loads(data[\"props\"][\"pageProps\"][\"componentProps\"][\"gdpClientCache\"])\n",
    "            property_data = property_data[list(property_data)[0]]['property']\n",
    "        else:\n",
    "            # Option 2: other times it's in Apollo cache\n",
    "            data = selector.css(\"script#hdpApolloPreloadedData::text\").get()\n",
    "            data = json.loads(json.loads(data)[\"apiCache\"])\n",
    "            property_data = next(\n",
    "                v[\"property\"] for k, v in data.items() if \"ForSale\" in k\n",
    "            )\n",
    "        results.append(property_data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run():\n",
    "    data = await scrape_properties(\n",
    "            [\"https://www.zillow.com/homedetails/1625-E-13th-St-APT-3K-Brooklyn-NY-11229/245001606_zpid/\"]\n",
    "        )\n",
    "    print(json.dumps(data, indent=2))\n",
    "\n",
    "# Execute the run function in an async context\n",
    "await run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Find Zillow Properties\n",
    "\n",
    "Our previous code for scraping Zillow can extract data from a property page. In this section, we'll explore finding real estate listings using Zillow's search bar. Here is how the search system works under the hood:\n",
    "\n",
    "0:00\n",
    "\n",
    "/0:35\n",
    "\n",
    "1Ã—\n",
    "\n",
    "Inspecting Zillow's search functionality with Chrome Dev tools (accessed via F12 key)\n",
    "\n",
    "Above, we can see that upon submitting a search query, a background request is sent to Zillow API for search. The search query includes the map coordinates, as well as other comprehensive details. However, few query parameters are actually required:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"searchQueryState\":{\n",
    "    \"pagination\":{},\n",
    "    \"usersSearchTerm\":\"New Haven, CT\",\n",
    "    \"mapBounds\":\n",
    "      {\n",
    "        \"west\":-73.03037621240235,\n",
    "        \"east\":-72.82781578759766,\n",
    "        \"south\":41.23043771298298,\n",
    "        \"north\":41.36611033618769\n",
    "      },\n",
    "    },\n",
    "  \"wants\": {\n",
    "    \"cat1\":[\"mapResults\"]\n",
    "  },\n",
    "  \"requestId\": 2\n",
    "}\n",
    "```\n",
    "\n",
    "The Zillow search API is really powerful and allows us to find listings in _any_ map area defined by two location points comprised of 4 direction values: north, west, south, and east:\n",
    "\n",
    "![illustration of drawing areas on maps using only two points](https://scrapfly.io/blog/content/images/how-to-scrape-zillow_two-points.svg)\n",
    "\n",
    "with these 4 values we can draw a square or a circle area at any point of the map!\n",
    "\n",
    "Let's replicate the login for finding properties by location to our Zillow scraping code using the latitude and longitude values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import time\n",
    "\n",
    "# we should use browser-like request headers to prevent being instantly blocked\n",
    "BASE_HEADERS = {\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "    \"Content-Type\": \"application/json\",  # Added Content-Type header\n",
    "}\n",
    "\n",
    "url = \"https://www.zillow.com/async-create-search-page-state\"\n",
    "body = {\n",
    "    \"searchQueryState\": {\n",
    "        \"pagination\": {},\n",
    "        \"usersSearchTerm\": \"New Haven, CT\",\n",
    "        # map coordinates that indicate New Haven city's area\n",
    "        \"mapBounds\": {\n",
    "            \"west\": -73.03037621240235,\n",
    "            \"east\": -72.82781578759766,\n",
    "            \"south\": 41.23043771298298,\n",
    "            \"north\": 41.36611033618769,\n",
    "        },\n",
    "    },\n",
    "    \"wants\": {\"cat1\": [\"listResults\", \"mapResults\"], \"cat2\": [\"total\"]},\n",
    "    \"requestId\": 2,\n",
    "}\n",
    "\n",
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    response = httpx.put(url, headers=BASE_HEADERS, data=json.dumps(body))\n",
    "    if response.status_code == 200:\n",
    "        break\n",
    "    elif attempt < max_retries - 1:\n",
    "        time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    else:\n",
    "        raise RuntimeError(f\"Request has been blocked. Status code: {response.status_code}, Response: {response.text}\")\n",
    "\n",
    "data = response.json()\n",
    "results = data[\"cat1\"][\"searchResults\"][\"mapResults\"]\n",
    "print(json.dumps(results, indent=2))\n",
    "print(f\"found {len(results)} property results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Scrape Zillow Search Pages?\n",
    "\n",
    "To scrape Zillow search, we need the geographical location details, which can be challenging to get. Therefore, we'll extract the location's geographical details from an easier user interface: search pages. To illustrate this, go to any search URL on Zillow, like [zillow.com/homes/New-Haven,-CT\\_rb/](https://www.zillow.com/homes/New-Haven,-CT_rb/). You fill find the geographical details hidden in the HTML:\n",
    "\n",
    "![capture of page source of Zillow's search pager](https://scrapfly.io/blog/content/images/how-to-scrape-zillow_page-source-search.svg)\n",
    "\n",
    "We can see query and geo data of this search hidden in a page source comment\n",
    "\n",
    "The geographical details exist in the script tag. Let's use it to scrape Zillow data from search pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import httpx\n",
    "from loguru import logger as log\n",
    "from parsel import Selector\n",
    "\n",
    "BASE_HEADERS = {\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "def _search(query: str, session: httpx.Client, filters: dict = None, categories=(\"cat1\", \"cat2\")):\n",
    "    \"\"\"base search function which is used by sale and rent search functions\"\"\"\n",
    "    html_response = session.get(f\"https://www.zillow.com/homes/{query}_rb/\")\n",
    "    assert html_response.status_code != 403, \"request is blocked\"\n",
    "    selector = Selector(html_response.text)\n",
    "    # find query data in script tags\n",
    "    script_data = json.loads(selector.xpath(\"//script[@id='__NEXT_DATA__']/text()\").get())\n",
    "    query_data = script_data[\"props\"][\"pageProps\"][\"searchPageState\"][\"queryState\"]\n",
    "    if filters:\n",
    "        query_data[\"filterState\"] = filters\n",
    "\n",
    "    # scrape search API\n",
    "    url = \"https://www.zillow.com/async-create-search-page-state\"\n",
    "    found = []\n",
    "    # cat1 - Agent Listings\n",
    "    # cat2 - Other Listings\n",
    "    for category in categories:\n",
    "        full_query = {\n",
    "            \"searchQueryState\": query_data,\n",
    "            \"wants\": {category: [\"mapResults\"]},\n",
    "            \"requestId\": random.randint(2, 10),\n",
    "        }\n",
    "        api_response = session.put(url, headers={\"content-type\": \"application/json\"}, json=full_query)\n",
    "        data = api_response.json()\n",
    "        _total = data[\"categoryTotals\"][category][\"totalResultCount\"]\n",
    "        if _total > 500:\n",
    "            log.warning(f\"query has more results ({_total}) than 500 result limit \")\n",
    "        else:\n",
    "            log.info(f\"found {_total} results for query: {query}\")\n",
    "        map_results = data[category][\"searchResults\"][\"mapResults\"]\n",
    "        found.extend(map_results)\n",
    "    return found\n",
    "\n",
    "def search_sale(query: str, session: httpx.Client):\n",
    "    \"\"\"search properties that are for sale\"\"\"\n",
    "    log.info(f\"scraping sale search for: {query}\")\n",
    "    return _search(query=query, session=session)\n",
    "\n",
    "def search_rent(query: str, session: httpx.Client):\n",
    "    \"\"\"search properties that are for rent\"\"\"\n",
    "    log.info(f\"scraping rent search for: {query}\")\n",
    "    filters = {\n",
    "        \"isForSaleForeclosure\": {\"value\": False},\n",
    "        \"isMultiFamily\": {\"value\": True},\n",
    "        \"isAllHomes\": {\"value\": False},\n",
    "        \"isAuction\": {\"value\": False},\n",
    "        \"isNewConstruction\": {\"value\": False},\n",
    "        \"isForRent\": {\"value\": False},\n",
    "        \"isLotLand\": {\"value\": False},\n",
    "        \"isManufactured\": {\"value\": False},\n",
    "        \"isForSaleByOwner\": {\"value\": False},\n",
    "        \"isComingSoon\": {\"value\": False},\n",
    "        \"isForSaleByAgent\": {\"value\": True},\n",
    "        \"price\": {\"max\": 1000000},\n",
    "        \"lot\": {\"min\": 5000},\n",
    "        \"beds\": {\"min\": 1},\n",
    "        \"baths\": {\"min\": 1}\n",
    "    }\n",
    "    return _search(query=query, session=session, filters=filters, categories=[\"cat1\"])\n",
    "\n",
    "def run():\n",
    "    limits = httpx.Limits(max_connections=5)\n",
    "    with httpx.Client(limits=limits, timeout=httpx.Timeout(15.0), headers=BASE_HEADERS) as session:\n",
    "        data = search_rent(\"Eureka,CA\", session)\n",
    "        with open('properties.geojson', 'w') as geojson_file:\n",
    "            geojson_data = {\n",
    "                \"type\": \"FeatureCollection\",\n",
    "                \"features\": [\n",
    "                    {\n",
    "                        \"type\": \"Feature\",\n",
    "                        \"geometry\": {\n",
    "                            \"type\": \"Point\",\n",
    "                            \"coordinates\": [result[\"latLong\"][\"longitude\"], result[\"latLong\"][\"latitude\"]],\n",
    "                        },\n",
    "                        \"properties\": result,\n",
    "                    }\n",
    "                    for result in data\n",
    "                ],\n",
    "            }\n",
    "            json.dump(geojson_data, geojson_file, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "from loguru import logger as log\n",
    "\n",
    "def convert_to_geojson(data):\n",
    "    features = []\n",
    "    for item in data:\n",
    "        if \"latLong\" in item and \"latitude\" in item[\"latLong\"] and \"longitude\" in item[\"latLong\"]:\n",
    "            coordinates = (item[\"latLong\"][\"longitude\"], item[\"latLong\"][\"latitude\"])\n",
    "            feature = Feature(\n",
    "                geometry=Point(coordinates),\n",
    "                properties={\n",
    "                    \"zpid\": item.get(\"zpid\"),\n",
    "                    \"statusType\": item.get(\"statusType\"),\n",
    "                    \"statusText\": item.get(\"statusText\"),\n",
    "                    \"price\": item.get(\"price\"),\n",
    "                    \"beds\": item.get(\"beds\"),\n",
    "                    \"baths\": item.get(\"baths\"),\n",
    "                    \"area\": item.get(\"area\"),\n",
    "                    \"address\": item.get(\"address\"),\n",
    "                    \"city\": item.get(\"hdpData\", {}).get(\"homeInfo\", {}).get(\"city\"),\n",
    "                    \"state\": item.get(\"hdpData\", {}).get(\"homeInfo\", {}).get(\"state\"),\n",
    "                    \"zipcode\": item.get(\"hdpData\", {}).get(\"homeInfo\", {}).get(\"zipcode\"),\n",
    "                    \"homeType\": item.get(\"hdpData\", {}).get(\"homeInfo\", {}).get(\"homeType\"),\n",
    "                }\n",
    "            )\n",
    "            features.append(feature)\n",
    "\n",
    "    return FeatureCollection(features)\n",
    "\n",
    "def search_by_bbox(bbox, session, filters=None):\n",
    "    url = \"https://www.zillow.com/async-create-search-page-state\"\n",
    "    query_data = {\n",
    "        \"pagination\": {},\n",
    "        \"mapBounds\": {\n",
    "            \"west\": bbox[0],\n",
    "            \"east\": bbox[2],\n",
    "            \"south\": bbox[1],\n",
    "            \"north\": bbox[3],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if filters:\n",
    "        query_data[\"filterState\"] = filters\n",
    "\n",
    "    full_query = {\n",
    "        \"searchQueryState\": query_data,\n",
    "        \"wants\": {\"cat1\": [\"mapResults\"]},\n",
    "        \"requestId\": random.randint(2, 10),\n",
    "    }\n",
    "\n",
    "    response = session.put(url, headers=BASE_HEADERS, json=full_query)\n",
    "    data = response.json()\n",
    "    return data[\"cat1\"][\"searchResults\"][\"mapResults\"]\n",
    "\n",
    "def run_geojson_search(input_geojson_file, output_geojson_file, filters=None):\n",
    "    geodf = gpd.read_file(input_geojson_file)\n",
    "    bbox = geodf.total_bounds\n",
    "\n",
    "    limits = httpx.Limits(max_connections=5)\n",
    "    with httpx.Client(limits=limits, timeout=httpx.Timeout(15.0), headers=BASE_HEADERS) as session:\n",
    "        results = search_by_bbox(bbox, session, filters)\n",
    "\n",
    "        geojson_data = convert_to_geojson(results)\n",
    "\n",
    "        with open(output_geojson_file, 'w') as geojson_file:\n",
    "            json.dump(geojson_data, geojson_file, indent=2)\n",
    "\n",
    "        log.info(f\"GeoJSON file '{output_geojson_file}' created with {len(geojson_data['features'])} features.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_geojson_file = \"/Users/maples/GitHub/Zillow-Scrape/grt_buffer_bbox_wgs84.geojson\"\n",
    "    output_geojson_file = \"output_properties03.geojson\"\n",
    "    run_geojson_search(input_geojson_file, output_geojson_file, filters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
